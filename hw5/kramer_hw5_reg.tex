\documentclass[11pt]{article}

% ==== PACKAGES ==== %
% \usepackage{fullpage}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{epic}
\usepackage{eepic}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{float}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{color}
\usepackage{bbm}
\usepackage[letterpaper, margin=1in]{geometry}

% ==== MARGINS ==== %
% \pagestyle{empty}
% \setlength{\oddsidemargin}{0in}
% \setlength{\textwidth}{6.8in}
% \setlength{\textheight}{9.5in}

\pagestyle{fancy}
\fancyhf{}
\rhead{ASEN 5044}
\lhead{Homework 1}
\rfoot{Page \thepage}

\newtheorem*{solution*}{Solution}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{corollary}[lemma]{Corollary}
\lstset{moredelim=[is][\bfseries]{[*}{*]}}

% ==== DOCUMENT PROPER ==== %
\begin{document}

\thispagestyle{empty}

% --- Header Box --- %
\newlength{\boxlength}\setlength{\boxlength}{\textwidth}
\addtolength{\boxlength}{-4mm}

\begin{center}\framebox{\parbox{\boxlength}{\bf
      Statistical Estimation \hfill Homework 5\\
      ASEN 5044 Fall 2018 \hfill Due Date: October 18, 2018\\
      Name: Andrew Kramer \hfill PhD Student
}}
\end{center}

\section*{Problem 1}
Consider two zero-mean uncorrelated random variables $W$ and $V$ with standard deviations $\sigma_w$ $\sigma_v$, respectively. What is the standard deviation of the random variable $X=W+V$?

\subparagraph*{}
The variance of $X$ can be expressed as
\begin{align*}
	\sigma_X^2 &= E(X^2)-E(X)^2 \\
	&= E((W+V)^2)-E(W+V)^2 \\
	&= E(W^2 + 2WV + V^2) - (E(W)+E(V))^2 \\
	&= E(W^2) + 2E(WV) + E(V^2) - E(W)^2 - 2E(W)E(V) - E(V)^2
\end{align*}
Because $W$ and $V$ are uncorrelated, $E(WV)=E(W)E(V)$. This means the above expression reduces to
\begin{align*}
	\sigma_X^2 &= E(W^2)-E(W)^2 + E(V^2) - E(V)^2 \\
	&= \sigma_W^2 + \sigma_V^2
\end{align*}
So the standard deviation of $X$ is $\sqrt{\sigma_W^2+\sigma_V^2}$.

\section*{Problem 2}
Consider two scalar RVs $X$ and $Y$.

\subsection*{Part a}
Prove that if $X$ and $Y$ are independent their correlation coefficient $\rho=0$.

\subparagraph*{}
For independent random variables $E(XY)=E(X)E(Y)$. Because of this their covariance $C_{XY}=E(XY)-E(X)E(Y)=0$. This means their correlation coefficient is
\begin{equation*}
	\rho = \frac{C_{XY}}{\sigma_x \sigma_y} = \frac{0}{\sigma_x \sigma_y} = 0
\end{equation*}

\subsection*{Part b}
Find an example of two RVs that are not independent but have a correlation coefficient of zero.

\subparagraph*{}
Assume $X=\mathcal{U}(-1,1)$ and $Y=X^2$. Because $\rho=\frac{C_{XY}}{\sigma_X \sigma_Y}$ we just need to show that 
\begin{equation*}
	C_{XY}=E(XY)-E(X)E(Y)=0
\end{equation*} to show $\rho=0$. From the definition of the uniform distribution we know that $E(X)=\frac{1}{2}(-1+1)=0$, so we know $E(X)E(Y)=0$. We can now find 
\begin{align*}
	E(XY) &= E(X^3) \\
	&= \int_{-1}^1 x^3dx \\
	&= \frac{1}{4} x^4 \Big|_{-1}^1 \\
	&= \frac{1}{4}-\frac{1}{4} = 0
\end{align*}
So because $C_{XY}=E(XY)-E(X)E(Y) = 0 - 0E(Y) = 0$, $\rho$ must also be equal to zero.

\subsection*{Part c}
Prove that if $Y$ is a linear function of $X$ then $\rho=\pm1$.

\subparagraph*{}
To show that $\rho=\pm 1$ when $Y$ is a linear function of $X$ we simply need to show that $|C_{XY}|=|\sigma_X \sigma_Y|$. We can do this by finding $E(Y)$, $E(Y^2)$, $E(XY)$, and $\sigma_Y$ in terms of $E(X)$, $E(X^2)$, and $\sigma_X$.
\begin{align*}
	E(X) &= \int_{-\infty}^{\infty}XdX = \frac{1}{2}X^2\Big|_{-\infty}^{\infty} \\
	E(X^2) &= \int_{-\infty}^{\infty}X^2dX = \frac{1}{3}X^3\Big|_{-\infty}^{\infty} \\
	E(Y) &= E(AX) + E(B) \\
	&= AE(X) + B \\
	E(Y^2) &= E((AX+B)^2) \\
	&= E(A^2X^2 + 2ABX + B^2) \\
	&= A^2E(X^2) + 2ABE(X) + B^2 \\
	E(XY) &= E(AX^2+BX) \\
	&= AE(X^2)+BE(X) \\
	\sigma_Y &= E(Y^2)-(E(Y))^2 \\
	&= A^2E(X^2) + 2ABE(X) + B^2 - (AE(X)+B)^2 \\
	&= A^2E(X^2) + 2ABE(X) + B^2 - A^2(E(X))^2 -2ABE(X)-B^2 \\
	&= A^2(E(X^2)-(E(X)^2)) \\
	&= A^2\sigma_X^2
\end{align*}
Given these preliminaries we can find 
\begin{align*}
	C_{XY} &= E(XY)-E(X)E(Y) \\
	&= AE(X^2)+BE(X)-E(X)(AE(X)+B) \\
	&= AE(X^2)+BE(X)-AE(X)^2 - BE(X) \\
	&= A(E(X^2)-E(X)^2) \\
	&= A\sigma_X^2 \\
	\sigma_X\sigma_Y &= \sigma_X \sqrt{A^2\sigma_X^2} \\
	&= A\sigma_X^2
\end{align*} 
All of this shows that when $Y$ is a linear function of $X$,
\begin{equation*}
	\rho = \frac{C_{XY}}{\sigma_X \sigma_Y} = \frac{A\sigma_X^2}{A\sigma_X^2}=1
\end{equation*}

\section*{Problem 3}
Consider the following function
\begin{equation*}
	f_{XY}=\begin{cases} ae^{-2x}e^{-3y} & x>0,\ y>0 \\ 0 & \text{otherwise} \end{cases}
\end{equation*}

\subsection*{Part a}
Find the value of $a$ so that $f_{XY}(x,y)$ is a valid joint probability density function.

\subparagraph*{}
Because $\int_X \int_Y f_{XY}dydx=1$ we can find $a$ by the following:
\begin{align*}
	1 &= \int_{-\infty}^\infty f_{XY} dy \\
	&= ae^{-2x}\int_0^\infty e^{-3y}dy \\
	&= -\frac{a}{3}e^{-2x}e^{-3y}\Big|_0^\infty \\
	&= \frac{a}{3}e^{-2x} \\
	\int_0^\infty \frac{a}{3}e^{-2x}dx &= -\frac{a}{6}e^{-2x}\Big|_0^\infty \\
	&= \frac{a}{6} \\
	a &= 6
\end{align*}

\subsection*{Part b}
Calculate $\bar{x}$ and $\bar{y}$.

\subparagraph*{}
To find $E(X)$ and $E(Y)$ we do the following:
\begin{align*}
	E(X) &= \int_{-\infty}^{\infty}xf_Xdx \\
	&= \int_{-\infty}^{\infty}x\int_{-\infty}^{\infty}f_{XY}dydx \\
	&= \int_{-\infty}^{\infty}2xe^{-2x}dx \\
	&= \frac{-2x-1}{2}e^{-2x}\Big|_0^\infty \\
	&= \frac{1}{2} \\
	E(Y) &= \int_{-\infty}^{\infty}yf_Ydy \\
	&= \int_{-\infty}^{\infty}y\int_{-\infty}^{\infty}f_{XY}dxdy \\
	&= \int_{-\infty}^{\infty}2ye^{-3y}dy \\
	&= \frac{-3y-1}{3}e^{-3y}\Big|_0^\infty \\
	&= \frac{1}{3}
\end{align*}

\subsection*{Part c}
Calculate $E(X^2)$, $E(Y^2)$, and $E(XY)$.

\subparagraph*{}
\begin{align*}
	E(X^2) &= \int_X x^2f_Xdx \\
	&= 2\int_X x^2e^{-2x}dx \\
	&= \frac{-2x^2-2x-1}{2}e^{-2x}\Big|_0^\infty \\
	&= \frac{1}{2} \\
	E(Y^2) &= \int_Y y^2f_Ydy \\
	&= 3 \int_Y y^2e^{-3y}dy \\
	&= \frac{-9y^2-6y-2}{9}e^{-3y}\Big|_0^\infty \\
	&= \frac{2}{9} \\
	E(XY) &= \int_Y\int_X xyf_{XY}dxdy \\
	&= 6\int_Y\int_X e^{-2x}e^{-3y}dxdy \\
	&= 6\int_Y\Big[\frac{-2x-1}{4}e^{-2x}ye^{-3y}\Big|_{x=0}^\infty dy \\
	&= \frac{6}{4}\int_Y ye^{-3y}dy \\
	&= \frac{6}{4}\frac{-3y-1}{9}e^{-3y}\Big|_{y=0}^\infty \\
	&= \frac{1}{6}
\end{align*}

\subsection*{Part d}
Calculate the autocorrelation matrix of the random vector $[X\ Y]^T$.

\subparagraph*{}
\begin{equation*}
	R_{XY} = \begin{bmatrix} E(X^2) & E(XY) \\ E(YX) & E(Y^2) \end{bmatrix}
	= \begin{bmatrix} 1/2 & 2/9 \\ 2/9 & 1/6 \end{bmatrix}
\end{equation*}

\subsection*{Part e}
Calculate the variance $\sigma_x^2$ and $\sigma_y^2$ and the covariance $C_{XY}$.

\subparagraph*{}
\begin{align*}
	\sigma_x^2 &= E(X^2)-E(X)^2 = \frac{1}{2}-\frac{1}{4} = \frac{1}{4} \\
	\sigma_y^2 &= E(Y^2)-E(Y)^2 = \frac{2}{9}-\frac{1}{9} = \frac{1}{9} \\
	C_{XY} &= E(XY) - E(X)E(Y) = \frac{1}{6}-\frac{1}{6} = 0
\end{align*}

\subsection*{Part f}
Calculate the autocovariance matrix of the random vector $[X\ Y]^T$.

\subparagraph*{}
\begin{equation*}
	C = \begin{bmatrix} \sigma_X^2 & C_{XY} \\ C_{XY} & \sigma_Y^2 \end{bmatrix} = \begin{bmatrix} 1/4 & 0 \\ 0 & 1/9 \end{bmatrix}
\end{equation*}

\subsection*{Part g}
Calculate the correlation coefficient between $X$ and $Y$.

\subparagraph*{}
Because the covariance $C_{XY}=0$ the correlation coefficient $\rho=\frac{C_{XY}}{\sigma_x\sigma_y}$ is also equal to zero.

\section*{Problem 4}
Prove the following two results used in lectire to derive the theoretical expectations for the Gaussian sampling experiment where $x\sim \mathcal{N}(\bar{x},\sigma_x^2)$, $e\sim \mathcal{N}(0,\sigma_e^2)$, and $y=cx+de$.

\subsection*{Part a}
$\text{cov}(X,Y) = E[(x-\bar{x})(y-\bar{y})]=E[XY]-\bar{x}\bar{y}$

\subparagraph*{}
\begin{align*}
	\text{cov}(X,Y) &= E[(x-\bar{x})(y-\bar{x})] \\
	&= E[xy - x\bar{y} - \bar{x}y + \bar{x}\bar{y}] \\
	&= E(xy) - \bar{y}E(x) - \bar{x}E(y) +\bar{x}\bar{y} \\
	&= E(xy) - \bar{y}\bar{x} - \bar{x}\bar{y} + \bar{x}\bar{y} \\
	&= E(xy) - \bar{x}\bar{y}
\end{align*}

\subsection*{Part b}
$\text{var}(Y) = E[(y-\bar{y})^2]=c^2\sigma_x^2+d^2\sigma_e^2$

\subparagraph*{}
The expected value of a linear combination of gaussians is $\sum_i c_i\bar{x}_i$ so the expected value of $y$ is $\bar{y}=c\bar{x}+d\bar{e}$. But since $\bar{e}=0$, $\bar{y}=c\bar{x}$. With this we can find $\text{var}(Y)$ as follows:
\begin{align*}
	\text{var}(Y) &= E((y-\bar{y})^2) \\
	&= E((cx+de-c\bar{x})^2) \\
	&= c^2E(x^2)+cdE(ex)-c^2\bar{x}E(x)+cdE(ex)+d^2E(e^2)-dc\bar{x}E(e)-\\
	&\qquad c^2\bar{x}E(x)-cd\bar{x}E(e)+c^2\bar{x}^2 \\
	& \text{since $x$ and $e$ are independent} \\
	&= c^2(E(x^2)-\bar{x}^2)+d^2E(e^2)-2cdE(e)E(x)-2dc\bar{x}E(e)\\
	& \text{and since $E(e)=0$} \\
	&= c^2(E(x^2)-\bar x^2) + d^2(E(e^2) - \bar{e}^2) \\
	&= c^2\sigma_x^2 + d^2\sigma_e^2
\end{align*}

\section*{Problem 5}
Consider two continuous random variables $x$ and $y$, where $y=\ln(x)$ and $x>0$. Derive analytical closed-form expressions for each of the following:

\subsection*{Part a}
$p(y)$ if $p(x) = \mathcal{U}[a,b]$ (i.e. if $x$ has a uniform pdf for $0<a\leq x \leq b$)

\subsection*{Part b}
$p(y)$ if $p(\frac{1}{x})=\mathcal{U}[c,d]$ (i.e. if $\frac{1}{x}$ has a uniform pdf $0<c\leq \frac{1}{x} \leq d$)

\subsection*{Part c}
$p(x)$ if $p(y) = \mathcal{U}[l,m]$ (i.e. if $y$ has a uniform pdf for $l\leq y \leq m$)

\subsection*{Part d}
$p(x)$ if $p(y) = \mathcal{N}(\mu_y,\sigma_y^2)$ (i.e. if $y$ has a Gaussian pdf with mean $\mu_y$ and variance $\sigma_y^2$)

\end{document}
